# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17TKNxSfkVB_b_1gODGWBmjgFgdiptobb
"""





#  STEP 0: Install dependencies
!pip install -q tensorflow transformers scikit-learn pandas
import pandas as pd
from sklearn.model_selection import train_test_split

# Load your dataset (replace with your actual path)
data = pd.read_json("intent_dataset.json")

# Extract features and labels
X = data['question']
y = data['intent']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print(" Data loaded and split successfully.")

#  STEP 1â€“8: Full BERT Classifier Pipeline
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report
import numpy as np
import pandas as pd


# Step 1: Encode labels
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_test_enc = label_encoder.transform(y_test)
num_classes = len(label_encoder.classes_)

# Step 2: Tokenize using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128, return_tensors="tf")
test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128, return_tensors="tf")

# Step 3: Create TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': train_encodings['input_ids'],
        'attention_mask': train_encodings['attention_mask']
    },
    tf.constant(y_train_enc)
)).shuffle(100).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': test_encodings['input_ids'],
        'attention_mask': test_encodings['attention_mask']
    },
    tf.constant(y_test_enc)
)).batch(16)

# Step 4: Compute class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_enc),
    y=y_train_enc
)
class_weights_dict = dict(enumerate(class_weights))

# Step 5: Load and compile BERT model
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)
steps_per_epoch = len(train_dataset)
optimizer, lr_schedule = create_optimizer(
    init_lr=3e-5,
    num_train_steps=steps_per_epoch * 10,
    num_warmup_steps=0
)
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Step 6: Train model
model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=10,
    class_weight=class_weights_dict
)

# Step 7: Evaluate
loss, accuracy = model.evaluate(test_dataset)
print(f"\n BERT Test Accuracy: {accuracy:.4f}")

# Step 8: Classification Report
y_pred_logits = model.predict(test_dataset).logits
y_pred = np.argmax(y_pred_logits, axis=1)
target_names = [str(cls) for cls in label_encoder.classes_]

print("\n BERT Classification Report:")
print(classification_report(y_test_enc, y_pred, target_names=target_names, zero_division=0))